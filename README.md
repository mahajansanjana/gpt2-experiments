# gpt2-experiments

So I fine-tuned GPT-2 on a bunch of psychedelic reports on Erowid 
(Ideally I'd have this running on the cloud somewhere but the GPT-2 is so large and compute-hungry and I haven't found a fast and cheap way to do this) 

# Acknowledgements 

* Max Woolf for his simplified GPT-2 fine-tuning package, [aitextgen](https://github.com/minimaxir/aitextgen)
* Matti Vuorre for compiling the wonderful [Erowid dataset](https://mvuorre.github.io/tmasc/articles/erowid/erowid.html)
* Slightly unrelated but also wanted to mention [this](https://chemicalyouth.org/visualising-erowid/) BEAUTIFUL visualization of Matti's dataset


