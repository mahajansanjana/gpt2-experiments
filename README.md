# gpt2-experiments
Fine-tuning GPT-2 on a bunch of psychedelic reports on Erowid
(Ideally I'd have this running on the cloud somewhere but the GPT-2 is so large and compute-hungry that I gave up)

# Acknowledgements 

Max Woolf for his simplified GPT-2 fine-tuning package, [aitextgen](https://github.com/minimaxir/aitextgen)

Matti Vuorre for compiling the wonderful [Erowid dataset](https://mvuorre.github.io/tmasc/articles/erowid/erowid.html)

Slightly unrelated but also wanted to mention [Chemical Youth](https://chemicalyouth.org/visualising-erowid/), which beautifully visualizes Matti's dataset


